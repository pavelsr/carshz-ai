#!/usr/bin/env python3
import os
import sys
import json
import shlex
import docker
import asyncio
import argparse
import datetime
#import logging
import pandas as pd
import oyaml as yaml
from pathlib import Path
from jinja2 import Environment, FileSystemLoader

'''
Что это? Cкрипт для тестирования неросетей для удаления фона с фотографии

Список конкретных проектов для тестирования задается в файле conf.yaml

Отличие от rmbgtst1 в асинхронности запуска
'''

#logging.basicConfig(level=logging.DEBUG)
MAX_CONCURRENT_CONTAINERS = 2

printable_task_prms = [
    "project_id", 
    "model_id", 
    "github", 
    "pypi", 
    "image", 
    "image_size_mb",
    "max_cpu_percentage",
    "avg_cpu_usage",
    "max_ram_usage",
    "run_time"
]

current_working_directory = os.getcwd()
cnfdict = {}

with open("conf.yaml") as stream:
    try:
        cnfdict = yaml.safe_load(stream)
    except yaml.YAMLError as exc:
        print(exc)

# cnfdict = { "results_folder": "results" }
client = docker.from_env()

s3_bucket_base = 'https://pub-7feff4ad9d804732bd5bf81661f02078.r2.dev'
thumbor_url = 'https://kemerovo.thumbor.linsec.dev/unsafe/0x100'
is_resfolder_bucket_root = True # set true if bucket root must be same as base_results_path

# Build docker run command
def generate_docker_run_command(config):
    command = ["docker", "run"]
    if "container_name" in config:
        command.extend(["--name", config["container_name"]])

    if "volumes" in config:
        for host_path, volume_config in config["volumes"].items():
            bind = volume_config["bind"]
            mode = volume_config.get("mode", "rw")
            command.extend(["-v", f"{host_path}:{bind}:{mode}"])

    if "working_dir" in config:
        command.extend(["-w", config["working_dir"]])

    command.append(config["image"])

    if "command" in config:
        command.extend(shlex.split(config["command"]))  # Use shlex.split to handle complex commands

    return " ".join(shlex.quote(arg) for arg in command)


def get_docker_tasks(cnfdict):
    #print(json.dumps(cnfdict, indent=4))
    projects = cnfdict["projects"]
    result = []
    for proj_id, project in projects.items():
        if "models" in project and project["models"]:
            for model_id in project["models"]:
                cmd = project["cmd"].format(
                    samples_folder = cnfdict["samples_folder"],
                    results_folder = cnfdict["results_folder"],
                    proj_id = proj_id,
                    model_id = model_id 
                    )
                
                # Create subfolder in results if it not exists
                # {results_folder}/{proj_id}/{model_id}
                results_folder = "{results_folder}/{proj_id}/{model_id}".format(
                    results_folder = cnfdict["results_folder"],
                    proj_id = proj_id,
                    model_id = model_id
                )

                container_name = "{}_{}".format(proj_id,model_id)
                task = {
                    "project_id": proj_id,
                    "model_id": model_id,
                    "task_id": "{}.{}".format(proj_id, model_id),
                    "image": project["image"],
                    "container_name": container_name,
                    "command": cmd,
                    "working_dir": cnfdict["wbdir"],
                    "volumes": { current_working_directory: {'bind': cnfdict["wbdir"], 'mode': 'rw'}},
                    "results_folder": results_folder,
                    "github": project["github"],
                    "pypi": project["pypi"]
                }

                task["stats_file"] = "{}/{}.json".format(cnfdict["results_folder"], task["task_id"])
                
                docker_image_size_bytes = client.images.get(project["image"]).attrs['Size']
                task["image_size_mb"] = docker_image_size_bytes / (1024 * 1024)
                task["image_size_mb"] = int(task["image_size_mb"])

                result.append(task)
        else:
            print("⚠️ Project {} has no models to test".format(proj_id))
    return result


async def monitor_container(container, verbose=True, round2int=True):
    from docker.models.containers import Container
    if not isinstance(container, Container):
        raise TypeError("The input must be a docker.models.containers.Container object.")


    # client = docker.from_env()
    # try:
    #     container = client.containers.get(container_name)
    # except docker.errors.NotFound:
    #     print(f"Container {container_name} not found.")
    #     return

    # print(f"Monitoring container: {container_name}")

    container.reload()

    max_cpu_percentage = 0
    max_memory_usage = 0
    total_cpu_usage = 0  # Accumulating CPU usage over time
    total_time = 0       # Accumulating time intervals
    
    while container.status == "running":
        stats = container.stats(stream=False)

        # Ensure the required keys exist in the stats dictionary
        cpu_stats = stats.get('cpu_stats', {})
        precpu_stats = stats.get('precpu_stats', {})

        if 'cpu_usage' in cpu_stats and 'cpu_usage' in precpu_stats and 'system_cpu_usage' in cpu_stats and 'system_cpu_usage' in precpu_stats:
            # Calculate CPU usage
            cpu_delta = cpu_stats['cpu_usage']['total_usage'] - precpu_stats['cpu_usage']['total_usage']
            system_delta = cpu_stats['system_cpu_usage'] - precpu_stats['system_cpu_usage']
            num_cpus = cpu_stats.get('online_cpus', 1)  # Default to 1 CPU if the key is missing
            cpu_percentage = (cpu_delta / system_delta) * num_cpus * 100 if system_delta > 0 else 0

            # Accumulate CPU usage and time interval
            total_cpu_usage += cpu_percentage
            total_time += 1  # Increment time interval by 1 second
            
            # Get memory usage
            mem_usage = stats['memory_stats']['usage'] / 1024 / 1024  # Convert to MB

            if verbose == True:
                print(f"{container.name} CPU Usage: {cpu_percentage:.2f}%, Memory Usage: {mem_usage:.2f} MB")
            
            # Update maximums
            max_cpu_percentage = max(max_cpu_percentage, cpu_percentage)
            max_memory_usage = max(max_memory_usage, mem_usage)

        await asyncio.sleep(1)
        container.reload()

    #container = client.inspect_container(container_name)
    #start_time_object = datetime.datetime.fromisoformat(container['State']['StartedAt'])
    #end_time_object = datetime.datetime.fromisoformat(container['State']['FinishedAt'])

    start_time_object = datetime.datetime.fromisoformat(container.attrs['State']['StartedAt'])
    end_time_object = datetime.datetime.fromisoformat(container.attrs['State']['FinishedAt'])
    start_time_unix = datetime.datetime.timestamp(start_time_object)
    end_time_unix = datetime.datetime.timestamp(end_time_object)
    run_time = end_time_unix - start_time_unix

    avg_cpu_usage = total_cpu_usage / total_time if total_time > 0 else 0
    
    if round2int:
        max_cpu_percentage = int(max_cpu_percentage)
        avg_cpu_usage = int(avg_cpu_usage)
        max_memory_usage = int(max_memory_usage)
        run_time = int(run_time)
        # time_s = round(time_s, 2)

    return {
        "max_cpu_percentage": max_cpu_percentage,
        "avg_cpu_usage": avg_cpu_usage,
        "max_ram_usage": max_memory_usage,
        "run_time": run_time
    }

async def run_and_monitor_container(task):
    client = docker.from_env()

    try:
        container = client.containers.get(task["container_name"])
        #if container.status == "running":
        #    container.stop()
        container.remove(force=True)
    except:
        pass
    

    try:
        #print(f"Starting container with image: {image_name}")
        container = client.containers.run(
            task["image"], 
            task["command"],
            name = task["container_name"],
            working_dir = task["working_dir"],
            volumes = task["volumes"],
            detach=True,
            #tty=True,
            #stdin_open=True,
            )
        # Monitor the container asynchronously
        container_stats = await monitor_container(container)
        task.update(container_stats)

        # Stop and clean up the container after monitoring
        #container.remove()
        #print(f"Container {container.name} removed.")
        return task

    except docker.errors.DockerException as e:
        print(f"Error: {e}")



async def limited_container_runner(tasks, limit):
    """
    Run multiple Docker containers with a limit on concurrent executions.
    """
    semaphore = asyncio.Semaphore(limit)
    results = []
    total_tasks = len(tasks)
    async def limited_task(index, task):
        async with semaphore:
            print(f"Task {index + 1}/{total_tasks} started ...")
            os.makedirs(task["results_folder"], exist_ok=True)
            print(generate_docker_run_command(task))
            if not os.path.isfile(task["stats_file"]):
                result = await run_and_monitor_container(task)
                results.append(result)
                print("Task {} finished, writing results to {} ".format(task["task_id"], task["stats_file"]))
                try:
                    pd.DataFrame.from_records(result).to_json(task["stats_file"], orient='records' )
                except:
                    print("⚠️ Some problem happened during writing, but execution will continue...")
            else:
                print("⚠️ Task {} was already done, skipping....".format(task["stats_file"]))

    await asyncio.gather(*(limited_task(index, task) for index, task in enumerate(tasks)))
    return results

async def main(docker_tasks):
    result = await limited_container_runner(docker_tasks, MAX_CONCURRENT_CONTAINERS)
    return result


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Script to test quality and perfomance of popular neural networks for background removal')

    parser.add_argument('-d', '--dryrun',
        action='store_true',
        help='Just print all run commands line-by-line')

    parser.add_argument('-v', '--verbose',
        action='store_true',
        help='Be more verbose in output')
    
    parser.add_argument('-t', '--test',
        action='store_true',
        help='Test on only one result')
    
    parser.add_argument('-p', '--publish',
        action='store_true',
        help='Just merge and publish results (generate index.html)')

    parser.add_argument('-i', '--input',
        default=cnfdict["samples_folder"],
        help='Input folder or file. Overrides "samples_folder" conf.yaml value')

    parser.add_argument('-o', '--output',
        default=cnfdict["results_folder"],
        help='Output folder or file. Overrides "results_folder" conf.yaml value')

    args = parser.parse_args()
    print(args)

    if args.input:
        cnfdict["samples_folder"] = args.input
    if args.output:
        cnfdict["results_folder"] = args.output
    

    if args.publish:
        print("Just publish")

        def merge_json_files(input_directory):
            merged_array = []
            # Iterate over all json files in the input_directory
            for filename in os.listdir(input_directory):
                if filename.endswith('.json'):
                    file_path = os.path.join(input_directory, filename)
                    # Load the JSON file
                    with open(file_path, 'r') as f:
                        data = json.load(f)
                        merged_array.append(data[0])  # Append the element to the array
            return merged_array


        def find_files_with_same_filename(root_dir, filename):
            """
            Recursively searches for files with a specific filename in a directory tree.
            """
            matches = []
            for dirpath, _, filenames in os.walk(root_dir):
                if filename in filenames:
                    matches.append(os.path.join(dirpath, filename))
            return matches
        
        
        def get_all_media_files_recursively(directory, only_first_dir=False):
            """
            Recursively collects all files in the given directory.
            """
            file_list = []
            for dirpath, _, filenames in os.walk(directory):
                if only_first_dir and len(file_list) > 1:
                    print("Test mode active, so results will not full")
                    break
                for filename in filenames:
                    if not filename.endswith('.json'):
                        file_list.append(os.path.join(dirpath, filename))
            return file_list


        def get_filelabel_dict(file_path, base_results_path=None, use_s3 = True):
            """
            Constructs a dictionary with file path and its relative label based on the root directory.
             :file_path - full relative path to result image (ususally *.png)
            :const_label - constant image part label
            """

            dir_path = os.path.dirname(file_path)
            label = os.path.relpath(dir_path, start=base_results_path)

            # For recursive 
            if len(file_path.split(os.sep)) > 4:
                label = os.path.join(*label.split(os.sep)[0:2]) 

            x = {
                "file": file_path,
                "label": label
            }

            if use_s3:
                if is_resfolder_bucket_root:
                    file_path = Path(file_path).relative_to(base_results_path)
                x['img_full_s3'] = "{}/{}".format(s3_bucket_base, file_path)
                x['img_preview'] = "{}/{}".format(thumbor_url, x['img_full_s3'])

            return x

        def get_gh_pypi_docker_formatters():
            return {
                'github': lambda x: f'<a href="https://github.com/{x}"><img src="assets/github.png"></a>',
                'pypi': lambda x: f'<a href="https://pypi.org/project/{x}"><img src="assets/python.png"></a>',
                'image': lambda x: f'{x} <a href="https://hub.docker.com/r/{x}"><img src="assets/docker.png"></a>'
            }

        media_results_basepath = cnfdict["results_folder"]
        
        print("Result media directory: ", media_results_basepath)
        
        sample_files = get_all_media_files_recursively(media_results_basepath, only_first_dir=args.test)
        print(f"Found {len(sample_files)} results media files.")

        result = []
        # Process each sample file
        for sample in sample_files:
            task_result = []
            stem = Path(sample).stem

            ### PROCESS ORIGINAL FILE (WILL BE FIRST EL)
            original_meta = {
                "file": sample,
                "label": "original",
                "is_original": True,
                "stem": stem
            }
            # TODO: upload to s3 original files also
            # original_meta['img_preview'] = "{}/{}".format(thumbor_url, original_meta['img_full_s3'])

            task_result.append(original_meta)
            ### END OF PROCESS ORIGINAL FILE

            expected_filename = f"{stem}.png"
            nn_res_files = find_files_with_same_filename(media_results_basepath, expected_filename)
            
            for nn_res_file in nn_res_files:
                # get_filelabel_dict return dict with props: file, label, img_full_s3, img_preview
                x = get_filelabel_dict(nn_res_file, base_results_path=media_results_basepath)
                x["stem"] = stem
                task_result.append(x)
            
            result.append(task_result)

            # result.append({
            #     "stem": stem,
            #     "original": sample,
            #     "nn_results": [get_filelabel_dict(f, base_results_path=media_results_basepath) for f in nn_res_files]
            # })

        if args.verbose:
            print(json.dumps(result, indent=4))


        # get perfomance metrics
        perfomance_stats_list = merge_json_files(media_results_basepath)

        df = pd.DataFrame.from_records(perfomance_stats_list)
        if args.verbose:
            print(df)

        perfomance_metrics_html = df[printable_task_prms].to_html(
            render_links=False,
            escape=False, 
            classes="tablesorter",
            formatters=get_gh_pypi_docker_formatters()
            )

        # Render HTML using Jinja2
        template_path = '.'
        env = Environment(loader=FileSystemLoader(template_path))
        template = env.get_template('index.html.jinja')
        # rendered_output = template.render(pics=result, perfomance_metrics=perfomance_metrics_html)
        rendered_output = template.render(result=result, perfomance_metrics=perfomance_metrics_html)

        if args.dryrun:
            #print(rendered_output)
            sys.exit("Script started in dryrun mode, so no index.html generated")
            sys.exit()
    
        output_file = 'index.html'
        with open(output_file, 'w', encoding='utf-8') as file:
            file.write(rendered_output)
        print(f"Rendered output saved to {output_file}")

        sys.exit()

    docker_tasks = get_docker_tasks(cnfdict)
    
    if args.verbose:
        print("Tasks to run : ", json.dumps(docker_tasks, indent=4))

    if args.dryrun:
        sys.exit("Script started in dryrun mode, exiting")
    
    result = asyncio.run(main(docker_tasks)) # can be [None] in case of error

    if args.verbose:
        print("Execution results : ", json.dumps(docker_tasks, indent=4))

    df = pd.DataFrame.from_records(result)
    if df.isna().all().all():
        sys.exit("⚠️ No results to write, seems like all tasks was done previously")
        # All intermediate results are also stored (see code in `limited_task` func)

    if args.verbose:
        df.to_json('last_result.json', orient='records')
        df = df[printable_task_prms]
        df.to_html('last_result.html')
        df.to_html('last_result_rich.html', render_links=False, escape=False, formatters=get_gh_pypi_docker_formatters())
